{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Python LLM API Playbook - Quick Start\n",
    "\n",
    "This notebook demonstrates how to use the `llm_playbook` package to interact with multiple LLM providers using a consistent interface.\n",
    "\n",
    "## Supported Providers\n",
    "\n",
    "| Provider | Models | Highlights |\n",
    "|----------|--------|------------|\n",
    "| **OpenAI** | GPT-4o, GPT-4, GPT-3.5 | Industry standard |\n",
    "| **Anthropic** | Claude 4, Claude 3.5 | Excellent reasoning |\n",
    "| **Google Gemini** | Gemini 2.0, 1.5 | Multimodal, free tier |\n",
    "| **Groq** | Llama, Mixtral | Ultra-fast inference |\n",
    "| **Ollama** | Local models | Privacy-first, offline |"
   ],
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, install the package and set up your API keys."
   ],
   "metadata": {
    "id": "setup"
   }
  },
  {
   "cell_type": "code",
   "source": "# Install the package (from GitHub)\n!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git\n\n# Or install dependencies directly\n# !pip install -q openai anthropic google-genai groq ollama python-dotenv",
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setup API Keys from Colab Secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set environment variables from Colab Secrets\n",
    "# Add your keys in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
    "try:\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    print(\"OPENAI_API_KEY not found in secrets\")\n",
    "\n",
    "try:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "except:\n",
    "    print(\"ANTHROPIC_API_KEY not found in secrets\")\n",
    "\n",
    "try:\n",
    "    os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
    "except:\n",
    "    print(\"GOOGLE_API_KEY not found in secrets\")\n",
    "\n",
    "try:\n",
    "    os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
    "except:\n",
    "    print(\"GROQ_API_KEY not found in secrets\")\n",
    "\n",
    "print(\"API keys configured!\")"
   ],
   "metadata": {
    "id": "api_keys"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Basic Usage\n",
    "\n",
    "All clients follow the same interface. Here's the simplest way to use them:"
   ],
   "metadata": {
    "id": "basic_usage"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llm_playbook import OpenAIClient, AnthropicClient, GeminiClient, GroqClient\n",
    "\n",
    "# OpenAI\n",
    "openai_client = OpenAIClient()\n",
    "response = openai_client.chat(\"What is machine learning in one sentence?\")\n",
    "print(\"OpenAI:\", response)"
   ],
   "metadata": {
    "id": "basic_openai"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Anthropic (Claude)\n",
    "claude_client = AnthropicClient()\n",
    "response = claude_client.chat(\"What is machine learning in one sentence?\")\n",
    "print(\"Claude:\", response)"
   ],
   "metadata": {
    "id": "basic_anthropic"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Google Gemini\n",
    "gemini_client = GeminiClient()\n",
    "response = gemini_client.chat(\"What is machine learning in one sentence?\")\n",
    "print(\"Gemini:\", response)"
   ],
   "metadata": {
    "id": "basic_gemini"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Groq (ultra-fast!)\n",
    "groq_client = GroqClient()\n",
    "response = groq_client.chat(\"What is machine learning in one sentence?\")\n",
    "print(\"Groq:\", response)"
   ],
   "metadata": {
    "id": "basic_groq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Using System Prompts\n",
    "\n",
    "Set the AI's behavior with a system prompt:"
   ],
   "metadata": {
    "id": "system_prompts"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "client = OpenAIClient()\n",
    "\n",
    "response = client.chat(\n",
    "    message=\"Explain what an API is\",\n",
    "    system_prompt=\"You are a teacher explaining concepts to a 10-year-old. Use simple language and fun analogies.\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response)"
   ],
   "metadata": {
    "id": "system_prompt_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Multi-turn Conversations\n",
    "\n",
    "Maintain conversation history across multiple exchanges:"
   ],
   "metadata": {
    "id": "multi_turn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from llm_playbook import OpenAIClient, ChatMessage\n",
    "\n",
    "client = OpenAIClient()\n",
    "system_prompt = \"You are a helpful astronomy expert. Be concise.\"\n",
    "\n",
    "# Start conversation\n",
    "history = []\n",
    "\n",
    "# Turn 1\n",
    "response1 = client.chat(\n",
    "    message=\"What's the closest star to Earth?\",\n",
    "    system_prompt=system_prompt,\n",
    "    history=history\n",
    ")\n",
    "print(\"Q: What's the closest star to Earth?\")\n",
    "print(f\"A: {response1}\\n\")\n",
    "\n",
    "# Add to history\n",
    "history.append(ChatMessage(role=\"user\", content=\"What's the closest star to Earth?\"))\n",
    "history.append(ChatMessage(role=\"assistant\", content=response1))\n",
    "\n",
    "# Turn 2 - follows up on context\n",
    "response2 = client.chat(\n",
    "    message=\"Does it have any planets?\",\n",
    "    system_prompt=system_prompt,\n",
    "    history=history\n",
    ")\n",
    "print(\"Q: Does it have any planets?\")\n",
    "print(f\"A: {response2}\")"
   ],
   "metadata": {
    "id": "multi_turn_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated for real-time output:"
   ],
   "metadata": {
    "id": "streaming"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "client = OpenAIClient()\n",
    "\n",
    "print(\"Streaming: \", end=\"\")\n",
    "for token in client.stream(\"Write a haiku about programming.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print()"
   ],
   "metadata": {
    "id": "streaming_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Detailed Responses\n",
    "\n",
    "Get usage stats and metadata along with the response:"
   ],
   "metadata": {
    "id": "detailed"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "client = OpenAIClient()\n",
    "\n",
    "response = client.chat_with_details(\n",
    "    message=\"What is Python?\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Content:\", response.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\", response.usage)\n",
    "print(\"Finish reason:\", response.finish_reason)"
   ],
   "metadata": {
    "id": "detailed_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Compare Providers\n",
    "\n",
    "Send the same prompt to multiple providers and compare:"
   ],
   "metadata": {
    "id": "compare"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "prompt = \"Explain recursion in one sentence.\"\n",
    "\n",
    "providers = [\n",
    "    (\"OpenAI\", OpenAIClient()),\n",
    "    (\"Anthropic\", AnthropicClient()),\n",
    "    (\"Gemini\", GeminiClient()),\n",
    "    (\"Groq\", GroqClient()),\n",
    "]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, client in providers:\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = client.chat(prompt)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"\\n{name} ({elapsed:.2f}s):\")\n",
    "        print(f\"  {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: Error - {e}\")"
   ],
   "metadata": {
    "id": "compare_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Using Different Models\n",
    "\n",
    "Specify a different model when initializing the client:"
   ],
   "metadata": {
    "id": "models"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# OpenAI with GPT-4o\n",
    "gpt4_client = OpenAIClient(model=\"gpt-4o\")\n",
    "\n",
    "# Anthropic with Claude 3 Haiku (faster, cheaper)\n",
    "haiku_client = AnthropicClient(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "# Gemini with 1.5 Pro\n",
    "gemini_pro = GeminiClient(model=\"gemini-1.5-pro\")\n",
    "\n",
    "# Groq with Mixtral\n",
    "mixtral_client = GroqClient(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "print(\"Clients initialized with custom models!\")"
   ],
   "metadata": {
    "id": "models_example"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "- Check out the [examples/](../examples/) directory for more usage patterns\n",
    "- Read the [docs/getting_api_keys.md](../docs/getting_api_keys.md) for API key setup\n",
    "- Explore the individual provider notebooks for detailed examples"
   ],
   "metadata": {
    "id": "next_steps"
   }
  }
 ]
}