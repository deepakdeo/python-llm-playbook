{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT Models Tutorial\n",
    "\n",
    "This notebook covers working with OpenAI's GPT models using the `llm_playbook` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up the OpenAI client\n",
    "- Basic chat completions\n",
    "- System prompts and message roles\n",
    "- Multi-turn conversations\n",
    "- Streaming responses\n",
    "- Generation parameters (temperature, max_tokens)\n",
    "- Response inspection (usage, model, finish_reason)\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| `gpt-4o` | Latest flagship model, multimodal |\n",
    "| `gpt-4o-mini` | Fast and affordable (default) |\n",
    "| `gpt-4-turbo` | Previous generation flagship |\n",
    "| `gpt-4` | Original GPT-4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the package and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key from Colab Secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Add your OPENAI_API_KEY in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
    "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat Completions\n",
    "\n",
    "The simplest way to use OpenAI - just send a message and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import OpenAIClient\n",
    "\n",
    "# Initialize the client (uses gpt-4o-mini by default)\n",
    "client = OpenAIClient()\n",
    "\n",
    "# Simple chat\n",
    "response = client.chat(\"What is machine learning in one sentence?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific model\n",
    "client_gpt4 = OpenAIClient(model=\"gpt-4o\")\n",
    "\n",
    "response = client_gpt4.chat(\"What makes GPT-4 different from GPT-3?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Prompts and Message Roles\n",
    "\n",
    "System prompts set the AI's behavior and personality. They're powerful for controlling responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system prompt\n",
    "response = client.chat(\"Explain what an API is\")\n",
    "print(\"Without system prompt:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system prompt - as a teacher\n",
    "response = client.chat(\n",
    "    message=\"Explain what an API is\",\n",
    "    system_prompt=\"You are a friendly teacher explaining concepts to a 10-year-old. Use simple words and fun analogies.\"\n",
    ")\n",
    "print(\"As a teacher for kids:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system prompt - as a technical expert\n",
    "response = client.chat(\n",
    "    message=\"Explain what an API is\",\n",
    "    system_prompt=\"You are a senior software architect. Be precise and use proper technical terminology.\"\n",
    ")\n",
    "print(\"As a technical expert:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-turn Conversations\n",
    "\n",
    "Maintain context across multiple exchanges using conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import ChatMessage\n",
    "\n",
    "# Initialize conversation history\n",
    "history = []\n",
    "system_prompt = \"You are a helpful astronomy expert. Keep answers concise.\"\n",
    "\n",
    "# Turn 1\n",
    "user_msg_1 = \"What's the closest star to Earth?\"\n",
    "response_1 = client.chat(user_msg_1, system_prompt=system_prompt, history=history)\n",
    "\n",
    "print(f\"User: {user_msg_1}\")\n",
    "print(f\"Assistant: {response_1}\\n\")\n",
    "\n",
    "# Add to history\n",
    "history.append(ChatMessage(role=\"user\", content=user_msg_1))\n",
    "history.append(ChatMessage(role=\"assistant\", content=response_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2 - follows up on context\n",
    "user_msg_2 = \"Does it have any planets?\"\n",
    "response_2 = client.chat(user_msg_2, system_prompt=system_prompt, history=history)\n",
    "\n",
    "print(f\"User: {user_msg_2}\")\n",
    "print(f\"Assistant: {response_2}\\n\")\n",
    "\n",
    "# Add to history\n",
    "history.append(ChatMessage(role=\"user\", content=user_msg_2))\n",
    "history.append(ChatMessage(role=\"assistant\", content=response_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3 - continues the context\n",
    "user_msg_3 = \"Could humans ever travel there?\"\n",
    "response_3 = client.chat(user_msg_3, system_prompt=system_prompt, history=history)\n",
    "\n",
    "print(f\"User: {user_msg_3}\")\n",
    "print(f\"Assistant: {response_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated for real-time output. Great for chat interfaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\"Write a haiku about programming.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()  # newline at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with system prompt\n",
    "print(\"Streaming with persona: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\n",
    "    message=\"Tell me a joke about Python\",\n",
    "    system_prompt=\"You are a stand-up comedian who loves programming humor.\"\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation Parameters\n",
    "\n",
    "Control the output with parameters like `temperature` and `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature (0.0) - deterministic, focused\n",
    "response_low = client.chat(\n",
    "    message=\"Give me a creative name for a coffee shop\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(f\"Temperature 0.0: {response_low}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High temperature (1.5) - more creative, varied\n",
    "response_high = client.chat(\n",
    "    message=\"Give me a creative name for a coffee shop\",\n",
    "    temperature=1.5\n",
    ")\n",
    "print(f\"Temperature 1.5: {response_high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiting output length with max_tokens\n",
    "response_short = client.chat(\n",
    "    message=\"Explain the theory of relativity\",\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f\"Limited to 50 tokens:\\n{response_short}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining parameters\n",
    "response = client.chat(\n",
    "    message=\"Write a product description for a smart water bottle\",\n",
    "    system_prompt=\"You are a marketing copywriter. Be enthusiastic but concise.\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Response Inspection\n",
    "\n",
    "Get detailed information about the response including token usage and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed response\n",
    "response = client.chat_with_details(\n",
    "    message=\"What is Python?\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"=== Response Details ===\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"\\nModel: {response.model}\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(f\"\\nToken usage:\")\n",
    "print(f\"  - Prompt tokens: {response.usage['prompt_tokens']}\")\n",
    "print(f\"  - Completion tokens: {response.usage['completion_tokens']}\")\n",
    "print(f\"  - Total tokens: {response.usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish reason examples\n",
    "# 'stop' = natural completion\n",
    "# 'length' = hit max_tokens limit\n",
    "\n",
    "# This will hit the length limit\n",
    "response = client.chat_with_details(\n",
    "    message=\"Write a 500-word essay about climate change\",\n",
    "    max_tokens=20\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.content}...\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(\"(Response was cut off due to max_tokens limit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "Tips for working effectively with OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 1: Use clear, specific prompts\n",
    "# Bad: \"Write something about dogs\"\n",
    "# Good: \"Write 3 bullet points about the health benefits of owning a dog\"\n",
    "\n",
    "response = client.chat(\n",
    "    \"Write 3 bullet points about the health benefits of owning a dog\",\n",
    "    max_tokens=150\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 2: Use system prompts for consistent behavior\n",
    "json_assistant = OpenAIClient()\n",
    "\n",
    "response = json_assistant.chat(\n",
    "    message=\"List 3 programming languages\",\n",
    "    system_prompt=\"You always respond in valid JSON format. No markdown, just pure JSON.\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 3: Handle potential errors\n",
    "try:\n",
    "    response = client.chat(\"Hello!\")\n",
    "    print(f\"Success: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Basic usage**: `client.chat(message)` for simple queries\n",
    "2. **System prompts**: Control behavior with `system_prompt` parameter\n",
    "3. **Multi-turn**: Maintain context with `history` parameter\n",
    "4. **Streaming**: Real-time output with `client.stream()`\n",
    "5. **Parameters**: Control output with `temperature` and `max_tokens`\n",
    "6. **Inspection**: Get metadata with `client.chat_with_details()`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [Anthropic notebook](02_anthropic.ipynb) to compare with Claude\n",
    "- Check out [06_comparison.ipynb](06_comparison.ipynb) for side-by-side comparisons\n",
    "- Explore the [examples/](../examples/) directory for more patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
