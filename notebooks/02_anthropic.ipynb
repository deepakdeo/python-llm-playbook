{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Claude Tutorial\n",
    "\n",
    "This notebook covers working with Anthropic's Claude models using the `llm_playbook` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up the Anthropic client\n",
    "- Basic messages API usage\n",
    "- System prompts (Claude's specialty!)\n",
    "- Multi-turn conversations\n",
    "- Streaming responses\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| `claude-sonnet-4-20250514` | Latest Sonnet - balanced (default) |\n",
    "| `claude-opus-4-20250514` | Most capable, best reasoning |\n",
    "| `claude-3-5-sonnet-20241022` | Previous Sonnet version |\n",
    "| `claude-3-haiku-20240307` | Fast and affordable |\n",
    "\n",
    "## Why Claude?\n",
    "\n",
    "- Excellent at following complex instructions\n",
    "- Strong reasoning capabilities\n",
    "- 200K token context window\n",
    "- Great for analysis and writing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the package and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key from Colab Secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Add your ANTHROPIC_API_KEY in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
    "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Messages API\n",
    "\n",
    "The simplest way to use Claude - send a message and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import AnthropicClient\n",
    "\n",
    "# Initialize the client (uses claude-sonnet-4 by default)\n",
    "client = AnthropicClient()\n",
    "\n",
    "# Simple chat\n",
    "response = client.chat(\"What is machine learning in one sentence?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Claude 3 Haiku for faster, cheaper responses\n",
    "haiku_client = AnthropicClient(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "response = haiku_client.chat(\"What is 2 + 2?\")\n",
    "print(f\"Haiku says: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Prompts\n",
    "\n",
    "Claude excels at following system prompts. This is where you define its behavior, personality, and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system prompt\n",
    "response = client.chat(\"Review this code: print('hello world')\")\n",
    "print(\"Without system prompt:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With detailed system prompt\n",
    "response = client.chat(\n",
    "    message=\"Review this code: print('hello world')\",\n",
    "    system_prompt=\"\"\"You are a senior Python code reviewer. For each code snippet:\n",
    "1. Identify any issues or improvements\n",
    "2. Rate it on a scale of 1-10\n",
    "3. Provide a brief suggestion\n",
    "\n",
    "Be constructive but honest.\"\"\"\n",
    ")\n",
    "print(\"With code reviewer persona:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude as a specific character\n",
    "response = client.chat(\n",
    "    message=\"Tell me about space exploration\",\n",
    "    system_prompt=\"You are Carl Sagan. Speak with wonder about the cosmos, use poetic language, and reference the 'pale blue dot' perspective.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-turn Conversations\n",
    "\n",
    "Maintain context across multiple exchanges. Claude has excellent memory within conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import ChatMessage\n",
    "\n",
    "# Initialize conversation\n",
    "history = []\n",
    "system = \"You are a helpful math tutor. Explain concepts step by step.\"\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"What is a derivative in calculus?\"\n",
    "a1 = client.chat(q1, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q1}\")\n",
    "print(f\"Tutor: {a1}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q1))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2 - follows up naturally\n",
    "q2 = \"Can you give me a simple example?\"\n",
    "a2 = client.chat(q2, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q2}\")\n",
    "print(f\"Tutor: {a2}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q2))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3 - Claude remembers the context\n",
    "q3 = \"What about the derivative of x squared?\"\n",
    "a3 = client.chat(q3, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q3}\")\n",
    "print(f\"Tutor: {a3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated. Essential for long responses and chat interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\"Write a haiku about artificial intelligence.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming a longer response\n",
    "print(\"Streaming story: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\n",
    "    message=\"Write a very short story (3 sentences) about a robot learning to paint.\",\n",
    "    system_prompt=\"You are a creative writer. Be vivid but concise.\",\n",
    "    max_tokens=150\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Responses\n",
    "\n",
    "Get token usage and metadata along with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat_with_details(\n",
    "    message=\"What is Python?\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"=== Response Details ===\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"\\nModel: {response.model}\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(f\"\\nToken usage:\")\n",
    "print(f\"  - Input tokens: {response.usage['input_tokens']}\")\n",
    "print(f\"  - Output tokens: {response.usage['output_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Claude's Strengths\n",
    "\n",
    "Some tasks where Claude particularly excels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex instruction following\n",
    "response = client.chat(\n",
    "    message=\"Explain quantum entanglement\",\n",
    "    system_prompt=\"\"\"Format your response as:\n",
    "## Simple Explanation\n",
    "[1-2 sentences a child could understand]\n",
    "\n",
    "## Technical Details\n",
    "[2-3 sentences with proper physics terminology]\n",
    "\n",
    "## Real-world Application\n",
    "[1 practical use case]\"\"\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output\n",
    "response = client.chat(\n",
    "    message=\"List 3 popular programming languages\",\n",
    "    system_prompt=\"Respond only in valid JSON format. No markdown code blocks, just raw JSON.\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thoughtful analysis\n",
    "response = client.chat(\n",
    "    message=\"What are the pros and cons of remote work?\",\n",
    "    system_prompt=\"Be balanced and consider multiple perspectives. Keep it concise with 2-3 points each.\",\n",
    "    max_tokens=300\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Basic usage**: `client.chat(message)` for simple queries\n",
    "2. **System prompts**: Claude excels at following detailed instructions\n",
    "3. **Multi-turn**: Maintain context with `history` parameter\n",
    "4. **Streaming**: Real-time output with `client.stream()`\n",
    "5. **Details**: Get usage stats with `client.chat_with_details()`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [Gemini notebook](03_gemini.ipynb) to compare with Google's model\n",
    "- Check out [06_comparison.ipynb](06_comparison.ipynb) for side-by-side comparisons\n",
    "- Explore Claude's long context capabilities with larger documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
