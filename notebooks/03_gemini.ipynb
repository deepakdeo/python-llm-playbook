{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini Tutorial\n",
    "\n",
    "This notebook covers working with Google's Gemini models using the `llm_playbook` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up the Gemini client with google-genai\n",
    "- Basic content generation\n",
    "- System instructions via config\n",
    "- Multi-turn conversations\n",
    "- Streaming responses\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| `gemini-2.0-flash` | Latest fast model (default) |\n",
    "| `gemini-1.5-pro` | Most capable, 1M context |\n",
    "| `gemini-1.5-flash` | Fast and efficient |\n",
    "| `gemini-1.5-flash-8b` | Smallest/fastest |\n",
    "\n",
    "## Why Gemini?\n",
    "\n",
    "- Generous free tier\n",
    "- Multimodal capabilities (images, video, audio)\n",
    "- Massive context window (up to 1M tokens)\n",
    "- Great for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the package and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key from Colab Secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Add your GOOGLE_API_KEY in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
    "# Get your key at: https://aistudio.google.com\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Content Generation\n",
    "\n",
    "The simplest way to use Gemini - send a prompt and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import GeminiClient\n",
    "\n",
    "# Initialize the client (uses gemini-2.0-flash by default)\n",
    "client = GeminiClient()\n",
    "\n",
    "# Simple generation\n",
    "response = client.chat(\"What is machine learning in one sentence?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific model\n",
    "pro_client = GeminiClient(model=\"gemini-1.5-pro\")\n",
    "\n",
    "response = pro_client.chat(\"Explain the difference between AI, ML, and deep learning.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Instructions\n",
    "\n",
    "Set the model's behavior with system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system prompt\n",
    "response = client.chat(\"Write a greeting\")\n",
    "print(\"Without system prompt:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system prompt - pirate persona\n",
    "response = client.chat(\n",
    "    message=\"Write a greeting\",\n",
    "    system_prompt=\"You are a friendly pirate. Use nautical terms and say 'Arrr' occasionally.\"\n",
    ")\n",
    "print(\"As a pirate:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system prompt - formal assistant\n",
    "response = client.chat(\n",
    "    message=\"Write a greeting\",\n",
    "    system_prompt=\"You are a formal British butler. Be polite, proper, and slightly old-fashioned.\"\n",
    ")\n",
    "print(\"As a butler:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-turn Conversations\n",
    "\n",
    "Maintain context across multiple exchanges with conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import ChatMessage\n",
    "\n",
    "# Initialize conversation\n",
    "history = []\n",
    "system = \"You are a helpful travel guide. Be enthusiastic about destinations!\"\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"I'm planning a trip to Japan. Where should I go?\"\n",
    "a1 = client.chat(q1, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Traveler: {q1}\")\n",
    "print(f\"Guide: {a1}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q1))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2 - follow up\n",
    "q2 = \"What's the best time to visit?\"\n",
    "a2 = client.chat(q2, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Traveler: {q2}\")\n",
    "print(f\"Guide: {a2}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q2))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3 - continues the context\n",
    "q3 = \"What about food recommendations?\"\n",
    "a3 = client.chat(q3, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Traveler: {q3}\")\n",
    "print(f\"Guide: {a3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated for real-time output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\"Write a haiku about the ocean.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with system prompt\n",
    "print(\"Streaming recipe: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\n",
    "    message=\"Give me a quick pasta recipe\",\n",
    "    system_prompt=\"You are a chef. Be concise - just list ingredients and 3-4 steps.\"\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generation Parameters\n",
    "\n",
    "Control the output with temperature and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature - more focused\n",
    "response = client.chat(\n",
    "    message=\"Name a color\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(f\"Low temperature: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High temperature - more creative\n",
    "response = client.chat(\n",
    "    message=\"Name a color\",\n",
    "    temperature=1.5\n",
    ")\n",
    "print(f\"High temperature: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With max_tokens limit\n",
    "response = client.chat(\n",
    "    message=\"Tell me about the history of computing\",\n",
    "    max_tokens=50\n",
    ")\n",
    "print(f\"Limited response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed Responses\n",
    "\n",
    "Get token usage and metadata along with the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat_with_details(\n",
    "    message=\"What is Python?\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"=== Response Details ===\")\n",
    "print(f\"Content: {response.content}\")\n",
    "print(f\"\\nModel: {response.model}\")\n",
    "print(f\"Finish reason: {response.finish_reason}\")\n",
    "print(f\"\\nToken usage: {response.usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gemini's Strengths\n",
    "\n",
    "Some tasks where Gemini excels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google product knowledge\n",
    "response = client.chat(\n",
    "    \"What's the best way to use Google Sheets for data analysis?\",\n",
    "    max_tokens=200\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factual queries with citations\n",
    "response = client.chat(\n",
    "    message=\"What are the main causes of climate change?\",\n",
    "    system_prompt=\"Be factual and cite scientific consensus. Keep it brief.\",\n",
    "    max_tokens=200\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output\n",
    "response = client.chat(\n",
    "    message=\"List 3 healthy breakfast options\",\n",
    "    system_prompt=\"Respond in JSON format only. No markdown.\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Basic usage**: `client.chat(message)` for simple queries\n",
    "2. **System instructions**: Control behavior with `system_prompt`\n",
    "3. **Multi-turn**: Maintain context with `history` parameter\n",
    "4. **Streaming**: Real-time output with `client.stream()`\n",
    "5. **Parameters**: Control with `temperature` and `max_tokens`\n",
    "6. **Details**: Get metadata with `client.chat_with_details()`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [Groq notebook](04_groq.ipynb) for ultra-fast inference\n",
    "- Check out [06_comparison.ipynb](06_comparison.ipynb) for side-by-side comparisons\n",
    "- Explore Gemini's multimodal capabilities for image analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
