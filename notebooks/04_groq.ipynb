{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq Tutorial - Ultra-Fast Inference\n",
    "\n",
    "This notebook covers working with Groq's lightning-fast LLM inference using the `llm_playbook` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Setting up the Groq client\n",
    "- Available models (Llama, Mixtral, Gemma)\n",
    "- Basic chat with incredibly fast responses\n",
    "- Streaming for real-time output\n",
    "\n",
    "## Available Models\n",
    "\n",
    "| Model | Description |\n",
    "|-------|-------------|\n",
    "| `llama-3.3-70b-versatile` | Latest Llama 3.3 (default) |\n",
    "| `llama-3.1-70b-versatile` | Llama 3.1 70B |\n",
    "| `llama-3.1-8b-instant` | Fast small model |\n",
    "| `mixtral-8x7b-32768` | Mixtral MoE |\n",
    "| `gemma2-9b-it` | Google Gemma 2 |\n",
    "\n",
    "## Why Groq?\n",
    "\n",
    "- **Blazing fast**: 10x faster than other providers\n",
    "- **LPU hardware**: Custom chips designed for LLM inference\n",
    "- **Free tier**: Generous limits for experimentation\n",
    "- **Open models**: Access to Llama, Mixtral, and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the package and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package\n",
    "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup API Key from Colab Secrets\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Add your GROQ_API_KEY in the Secrets pane (ðŸ”‘ icon in left sidebar)\n",
    "# Get your key at: https://console.groq.com\n",
    "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
    "print(\"API key configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat - Feel the Speed!\n",
    "\n",
    "Groq's main selling point is speed. Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llm_playbook import GroqClient\n",
    "\n",
    "# Initialize the client (uses llama-3.3-70b-versatile by default)\n",
    "client = GroqClient()\n",
    "\n",
    "# Time the response\n",
    "start = time.time()\n",
    "response = client.chat(\"What is machine learning in one sentence?\")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nTime: {elapsed:.2f} seconds\")\n",
    "print(\"That's FAST! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with a longer response\n",
    "start = time.time()\n",
    "response = client.chat(\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    max_tokens=200\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nTime: {elapsed:.2f} seconds for ~200 tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Available Models\n",
    "\n",
    "Groq hosts several open-source models. Let's try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.3 70B (default)\n",
    "llama_client = GroqClient(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "response = llama_client.chat(\"What makes you different from other language models?\")\n",
    "print(\"Llama 3.3 70B:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.1 8B - even faster!\n",
    "small_client = GroqClient(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "start = time.time()\n",
    "response = small_client.chat(\"What is 2+2? Just the number.\")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Llama 3.1 8B: {response}\")\n",
    "print(f\"Time: {elapsed:.3f} seconds - instant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixtral 8x7B\n",
    "mixtral_client = GroqClient(model=\"mixtral-8x7b-32768\")\n",
    "\n",
    "response = mixtral_client.chat(\"Write a haiku about speed.\")\n",
    "print(\"Mixtral 8x7B:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemma 2 9B\n",
    "gemma_client = GroqClient(model=\"gemma2-9b-it\")\n",
    "\n",
    "response = gemma_client.chat(\"What's unique about Google's Gemma model?\")\n",
    "print(\"Gemma 2 9B:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompts\n",
    "\n",
    "Control the model's behavior with system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code assistant persona\n",
    "response = client.chat(\n",
    "    message=\"How do I read a CSV file?\",\n",
    "    system_prompt=\"You are a Python expert. Give brief, practical code examples. No explanations, just code.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON output mode\n",
    "response = client.chat(\n",
    "    message=\"List 3 colors\",\n",
    "    system_prompt=\"Respond only in valid JSON format. No markdown, no explanation.\",\n",
    "    temperature=0.0\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-turn Conversations\n",
    "\n",
    "Maintain context across multiple exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import ChatMessage\n",
    "\n",
    "history = []\n",
    "system = \"You are a helpful coding tutor. Be concise.\"\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"What is a Python list?\"\n",
    "a1 = client.chat(q1, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q1}\")\n",
    "print(f\"Tutor: {a1}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q1))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2\n",
    "q2 = \"How do I add items to it?\"\n",
    "a2 = client.chat(q2, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q2}\")\n",
    "print(f\"Tutor: {a2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming - Real-time Output\n",
    "\n",
    "Stream tokens as they're generated. Even with Groq's speed, streaming provides better UX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\"Write a limerick about programming.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming a longer response\n",
    "print(\"Streaming story:\\n\")\n",
    "\n",
    "for token in client.stream(\n",
    "    message=\"Write a 4-sentence story about a robot.\",\n",
    "    max_tokens=150\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Speed Comparison\n",
    "\n",
    "Let's benchmark different models on Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"llama-3.1-8b-instant\", \"Llama 3.1 8B\"),\n",
    "    (\"gemma2-9b-it\", \"Gemma 2 9B\"),\n",
    "    (\"mixtral-8x7b-32768\", \"Mixtral 8x7B\"),\n",
    "    (\"llama-3.3-70b-versatile\", \"Llama 3.3 70B\"),\n",
    "]\n",
    "\n",
    "prompt = \"What is Python? Answer in one sentence.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_id, name in models:\n",
    "    try:\n",
    "        test_client = GroqClient(model=model_id)\n",
    "        start = time.time()\n",
    "        response = test_client.chat(prompt)\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"\\n{name} ({elapsed:.2f}s):\")\n",
    "        print(f\"  {response[:100]}...\" if len(response) > 100 else f\"  {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use Groq\n",
    "\n",
    "Groq is ideal for:\n",
    "\n",
    "- **Real-time applications**: Chatbots, live coding assistants\n",
    "- **High-throughput**: Processing many requests quickly\n",
    "- **Prototyping**: Fast iteration during development\n",
    "- **Cost-sensitive**: Free tier + open models\n",
    "\n",
    "Consider other providers when:\n",
    "- You need the absolute best quality (try Claude or GPT-4)\n",
    "- You need multimodal capabilities (images, audio)\n",
    "- You need very long context windows (1M+ tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Speed**: Groq is incredibly fast - often sub-second responses\n",
    "2. **Models**: Access to Llama, Mixtral, Gemma, and more\n",
    "3. **Usage**: Same familiar interface as other providers\n",
    "4. **Streaming**: Real-time output for better UX\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [Ollama notebook](05_ollama.ipynb) for local LLM inference\n",
    "- Check out [06_comparison.ipynb](06_comparison.ipynb) for side-by-side comparisons\n",
    "- Explore Groq for building real-time AI applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
