{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Tutorial - Local LLMs\n",
    "\n",
    "This notebook covers running LLMs locally on your machine using Ollama and the `llm_playbook` package.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Installing Ollama on macOS/Linux/Windows\n",
    "- Pulling and managing models\n",
    "- Basic chat with local models\n",
    "- Listing available local models\n",
    "- Benefits of running locally\n",
    "\n",
    "## Popular Models\n",
    "\n",
    "| Model | Size | Use Case |\n",
    "|-------|------|----------|\n",
    "| `llama3.2` | 2-3GB | General purpose (default) |\n",
    "| `llama3.2:1b` | ~1GB | Fastest, low resource |\n",
    "| `mistral` | ~4GB | Great quality/speed balance |\n",
    "| `codellama` | ~4GB | Code generation |\n",
    "| `phi3` | ~2GB | Microsoft's small model |\n",
    "| `gemma2` | ~5GB | Google's open model |\n",
    "\n",
    "## Why Ollama?\n",
    "\n",
    "- **Privacy**: Data never leaves your machine\n",
    "- **Offline**: Works without internet\n",
    "- **Free**: No API costs, no rate limits\n",
    "- **Fast**: No network latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, you need to install Ollama on your local machine.\n",
    "\n",
    "### macOS\n",
    "```bash\n",
    "# Using Homebrew\n",
    "brew install ollama\n",
    "\n",
    "# Or download from https://ollama.com\n",
    "```\n",
    "\n",
    "### Linux\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "### Windows\n",
    "Download the installer from [ollama.com](https://ollama.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Models\n",
    "\n",
    "Before using a model, you need to download it. Run these in your terminal:\n",
    "\n",
    "```bash\n",
    "# Pull the default model (Llama 3.2)\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Pull a smaller/faster model\n",
    "ollama pull llama3.2:1b\n",
    "\n",
    "# Pull other models\n",
    "ollama pull mistral\n",
    "ollama pull codellama\n",
    "ollama pull phi3\n",
    "```\n",
    "\n",
    "Models are downloaded once and cached locally (~2-8GB each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Ollama\n",
    "\n",
    "Ollama runs as a background service. It usually starts automatically, but you can start it manually:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "The API runs at `http://localhost:11434` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "**Note**: This notebook requires Ollama running locally. It won't work in Google Colab (which runs in the cloud).\n",
    "\n",
    "Run this notebook on your local machine with Jupyter installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (if not already installed)\n",
    "!pip install -q git+https://github.com/deepakdeo/python-llm-playbook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is running\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        print(\"Ollama is running!\")\n",
    "        print(f\"Available models: {[m['name'] for m in models]}\")\n",
    "    else:\n",
    "        print(\"Ollama responded but with an error\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Ollama is not running!\")\n",
    "    print(\"Start it with: ollama serve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat with Local Models\n",
    "\n",
    "Using Ollama is just like using any other provider - no API key needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import OllamaClient\n",
    "\n",
    "# Initialize the client (uses llama3.2 by default)\n",
    "client = OllamaClient()\n",
    "\n",
    "# Simple chat\n",
    "response = client.chat(\"What is machine learning in one sentence?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a specific model\n",
    "mistral_client = OllamaClient(model=\"mistral\")\n",
    "\n",
    "response = mistral_client.chat(\"Write a haiku about coding.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Listing Local Models\n",
    "\n",
    "See what models you have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "models = client.list_models()\n",
    "\n",
    "print(\"Installed models:\")\n",
    "for model in models:\n",
    "    name = model.get('name', 'unknown')\n",
    "    size = model.get('size', 0) / (1024**3)  # Convert to GB\n",
    "    print(f\"  - {name} ({size:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Prompts\n",
    "\n",
    "Control the model's behavior with system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default response\n",
    "response = client.chat(\"Explain APIs\")\n",
    "print(\"Default:\")\n",
    "print(response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With system prompt\n",
    "response = client.chat(\n",
    "    message=\"Explain APIs\",\n",
    "    system_prompt=\"You explain things like a pirate. Say 'Arrr' and use nautical terms.\"\n",
    ")\n",
    "print(\"As a pirate:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-turn Conversations\n",
    "\n",
    "Maintain context across multiple exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_playbook import ChatMessage\n",
    "\n",
    "history = []\n",
    "system = \"You are a helpful Python tutor. Be concise.\"\n",
    "\n",
    "# Turn 1\n",
    "q1 = \"What is a dictionary in Python?\"\n",
    "a1 = client.chat(q1, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q1}\")\n",
    "print(f\"Tutor: {a1}\\n\")\n",
    "\n",
    "history.append(ChatMessage(role=\"user\", content=q1))\n",
    "history.append(ChatMessage(role=\"assistant\", content=a1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2 - follows context\n",
    "q2 = \"How do I add a new key?\"\n",
    "a2 = client.chat(q2, system_prompt=system, history=history)\n",
    "\n",
    "print(f\"Student: {q2}\")\n",
    "print(f\"Tutor: {a2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses\n",
    "\n",
    "Stream tokens as they're generated for real-time output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming: \", end=\"\")\n",
    "\n",
    "for token in client.stream(\"Write a limerick about Python.\"):\n",
    "    print(token, end=\"\", flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code Generation with CodeLlama\n",
    "\n",
    "If you have CodeLlama installed, it's great for coding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try CodeLlama if available\n",
    "try:\n",
    "    code_client = OllamaClient(model=\"codellama\")\n",
    "    \n",
    "    response = code_client.chat(\n",
    "        message=\"Write a Python function to check if a number is prime\",\n",
    "        system_prompt=\"You are a code assistant. Only output code, no explanations.\"\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"CodeLlama not available: {e}\")\n",
    "    print(\"Pull it with: ollama pull codellama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benefits of Running Locally\n",
    "\n",
    "### Privacy\n",
    "- Your data never leaves your machine\n",
    "- Great for sensitive information\n",
    "- No logging by third parties\n",
    "\n",
    "### Cost\n",
    "- Completely free to use\n",
    "- No API costs or rate limits\n",
    "- Use as much as you want\n",
    "\n",
    "### Speed\n",
    "- No network latency\n",
    "- Fast for local hardware (especially with GPU)\n",
    "- Works offline\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Size | RAM Required | GPU Optional |\n",
    "|------------|--------------|-------------|\n",
    "| 1-3B | 4-8GB | Helpful |\n",
    "| 7-8B | 8-16GB | Recommended |\n",
    "| 13B+ | 16-32GB | Highly recommended |\n",
    "| 70B | 48GB+ | Required |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Local vs Cloud\n",
    "\n",
    "| Feature | Local (Ollama) | Cloud (OpenAI, etc.) |\n",
    "|---------|----------------|---------------------|\n",
    "| Privacy | ✅ Full control | ❌ Data sent to servers |\n",
    "| Cost | ✅ Free | ❌ Per-token pricing |\n",
    "| Quality | ⚠️ Good for open models | ✅ Best models |\n",
    "| Speed | ⚠️ Hardware dependent | ✅ Optimized infrastructure |\n",
    "| Offline | ✅ Works offline | ❌ Requires internet |\n",
    "| Setup | ⚠️ Requires installation | ✅ Just API key |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "1. **Installation**: Install Ollama on macOS, Linux, or Windows\n",
    "2. **Models**: Pull models with `ollama pull <model>`\n",
    "3. **Usage**: Same interface as cloud providers\n",
    "4. **Benefits**: Privacy, cost, and offline capability\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try the [comparison notebook](06_comparison.ipynb) to see Ollama vs cloud providers\n",
    "- Explore different models for different tasks\n",
    "- Consider Ollama for privacy-sensitive applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
